{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de67cc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from proxy_auth_plugin import create_proxy_auth_extension\n",
    "from proxies import PROXIES\n",
    "\n",
    "# Archivos\n",
    "INPUT_FILE = \"a_scrapear.csv\"\n",
    "OUTPUT_FILE = \"scrapeado_final.csv\"\n",
    "CHECKPOINT_FILE = \"checkpoint.txt\"\n",
    "\n",
    "# Verificar si archivo original existe\n",
    "if not os.path.exists(INPUT_FILE):\n",
    "    raise FileNotFoundError(f\"‚ùå No se encontr√≥ el archivo {INPUT_FILE}\")\n",
    "\n",
    "# Cargar dataset principal\n",
    "df = pd.read_csv(INPUT_FILE)\n",
    "\n",
    "# Limitar n√∫mero de URLs\n",
    "n_urls = 167271\n",
    "df = df.head(n_urls)\n",
    "\n",
    "# A√±adir columna si no existe\n",
    "if 'description_full' not in df.columns:\n",
    "    df['description_full'] = None\n",
    "\n",
    "# Reanudar desde checkpoint si existe\n",
    "start_index = 0\n",
    "if os.path.exists(CHECKPOINT_FILE):\n",
    "    with open(CHECKPOINT_FILE, \"r\") as f:\n",
    "        start_index = int(f.read().strip())\n",
    "    print(f\"‚è∏Ô∏è Reanudando scraping desde √≠ndice {start_index}\")\n",
    "else:\n",
    "    print(f\"üîÅ Iniciando scraping desde el √≠ndice {start_index}\")\n",
    "\n",
    "# Si ya existe el archivo de salida, cargarlo y conservar lo anterior\n",
    "if os.path.exists(OUTPUT_FILE):\n",
    "    df_prev = pd.read_csv(OUTPUT_FILE)\n",
    "    df.loc[:len(df_prev)-1, 'description_full'] = df_prev['description_full']\n",
    "    print(f\"üìÇ Archivo existente {OUTPUT_FILE} cargado con {len(df_prev)} filas ya scrapeadas.\")\n",
    "\n",
    "\n",
    "# Lista de user-agents\n",
    "USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.4 Safari/605.1.15\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:124.0) Gecko/20100101 Firefox/124.0\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 13_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:123.0) Gecko/20100101 Firefox/123.0\",\n",
    "    \"Mozilla/5.0 (Windows NT 11.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edg/123.0.2420.65 Chrome/123.0.0.0 Safari/537.36\",\n",
    "]\n",
    "\n",
    "# Inicializador de driver\n",
    "def init_driver(proxy, user_agent):\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--incognito\")\n",
    "    options.add_argument(f\"user-agent={user_agent}\")\n",
    "\n",
    "    prefs = {\n",
    "        \"profile.managed_default_content_settings.images\": 2,\n",
    "        \"profile.managed_default_content_settings.stylesheets\": 2,\n",
    "        \"profile.managed_default_content_settings.fonts\": 2\n",
    "    }\n",
    "    options.add_experimental_option(\"prefs\", prefs)\n",
    "\n",
    "    plugin_path = create_proxy_auth_extension(\n",
    "        proxy_host=proxy[\"host\"],\n",
    "        proxy_port=proxy[\"port\"],\n",
    "        proxy_username=proxy[\"username\"],\n",
    "        proxy_password=proxy[\"password\"]\n",
    "    )\n",
    "    options.add_extension(plugin_path)\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    return driver, plugin_path\n",
    "\n",
    "# Scraping function\n",
    "def scrape_job_description_selenium(driver, url):\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        WebDriverWait(driver, 2.5).until(\n",
    "            EC.presence_of_element_located(\n",
    "                (By.CSS_SELECTOR, 'section.adp-body.mx-4.text-sm.md\\\\:mx-0.md\\\\:text-base')\n",
    "            )\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            show_more_button = driver.find_element(\n",
    "                By.CSS_SELECTOR,\n",
    "                \"a.font-bold.bg-white.inline-block.px-8.pt-4.text-adzuna-green-500.rounded-t-lg.hover\\\\:underline\"\n",
    "            )\n",
    "            show_more_button.click()\n",
    "            print(\"üîò 'Show Full Description' presionado.\")\n",
    "            time.sleep(random.uniform(0.2, 1))\n",
    "        except NoSuchElementException:\n",
    "            print(\"‚ö†Ô∏è Bot√≥n 'Show Full Description' no encontrado.\")\n",
    "\n",
    "        section = driver.find_element(By.CSS_SELECTOR, 'section.adp-body.mx-4.text-sm.md\\\\:mx-0.md\\\\:text-base')\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView();\", section)\n",
    "        time.sleep(random.uniform(0.5, 1.5))\n",
    "\n",
    "        description_html = section.get_attribute('innerHTML')\n",
    "        description_text = description_html.replace('<br>', '\\n')\n",
    "\n",
    "        if not description_text.strip():\n",
    "            elements = section.find_elements(By.CSS_SELECTOR, \"p, li\")\n",
    "            if not elements:\n",
    "                print(f\"‚ö†Ô∏è No se encontraron <p> o <li> en {url}\")\n",
    "                return None\n",
    "            description_text = \"\\n\".join([el.text.strip() for el in elements if el.text.strip()])\n",
    "\n",
    "        if not description_text.strip():\n",
    "            print(f\"‚ö†Ô∏è Descripci√≥n vac√≠a en {url}\")\n",
    "            return None\n",
    "\n",
    "        return description_text\n",
    "\n",
    "    except (TimeoutException, NoSuchElementException, ValueError) as e:\n",
    "        print(f\"‚ö†Ô∏è Error en {url}: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error inesperado: {e}\")\n",
    "        return None\n",
    "\n",
    "# Variables de control\n",
    "driver = None\n",
    "plugin_path = None\n",
    "restart_interval = 5\n",
    "save_interval = 10\n",
    "sleep_range = (0.5, 3)\n",
    "counter = 0\n",
    "\n",
    "# Loop principal de scraping\n",
    "for idx in range(start_index, len(df)):\n",
    "    url = df.at[idx, \"redirect_url\"]\n",
    "    if pd.notnull(url):\n",
    "        if counter % restart_interval == 0:\n",
    "            if driver:\n",
    "                driver.quit()\n",
    "                if plugin_path and os.path.exists(plugin_path):\n",
    "                    os.remove(plugin_path)\n",
    "            proxy = random.choice(PROXIES)\n",
    "            user_agent = random.choice(USER_AGENTS)\n",
    "            driver, plugin_path = init_driver(proxy, user_agent)\n",
    "            print(f\"üîÅ Driver reiniciado con proxy {proxy['host']}\")\n",
    "\n",
    "        description = scrape_job_description_selenium(driver, url)\n",
    "        df.at[idx, 'description_full'] = description\n",
    "        print(f\"[{idx}] URL procesada: {'‚úÖ' if description else '‚ùå'}\")\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "        # Guardado parcial\n",
    "        if counter % save_interval == 0:\n",
    "            df.to_csv(OUTPUT_FILE, index=False)\n",
    "            with open(CHECKPOINT_FILE, \"w\") as f:\n",
    "                f.write(str(idx + 1))\n",
    "            print(f\"üíæ Guardado parcial en √≠ndice {idx}\")\n",
    "\n",
    "        time.sleep(random.uniform(*sleep_range))\n",
    "\n",
    "# Limpieza final\n",
    "if driver:\n",
    "    driver.quit()\n",
    "if plugin_path and os.path.exists(plugin_path):\n",
    "    os.remove(plugin_path)\n",
    "\n",
    "df.to_csv(OUTPUT_FILE, index=False)\n",
    "with open(CHECKPOINT_FILE, \"w\") as f:\n",
    "    f.write(str(len(df)))\n",
    "\n",
    "print(\"‚úÖ Scraping finalizado y guardado.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mi_entorno",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
