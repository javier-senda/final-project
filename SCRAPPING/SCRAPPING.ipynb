{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de67cc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from proxy_auth_plugin import create_proxy_auth_extension\n",
    "\n",
    "# Archivos\n",
    "INPUT_FILE = \"a_scrapear.csv\"\n",
    "OUTPUT_FILE = \"scrapeado_final.csv\"\n",
    "CHECKPOINT_FILE = \"checkpoint.txt\"\n",
    "\n",
    "# Verificar si archivo original existe\n",
    "if not os.path.exists(INPUT_FILE):\n",
    "    raise FileNotFoundError(f\"‚ùå No se encontr√≥ el archivo {INPUT_FILE}\")\n",
    "\n",
    "# Cargar dataset principal\n",
    "df = pd.read_csv(INPUT_FILE)\n",
    "\n",
    "# Limitar n√∫mero de URLs\n",
    "n_urls = 167271\n",
    "df = df.head(n_urls)\n",
    "\n",
    "# A√±adir columna si no existe\n",
    "if 'description_full' not in df.columns:\n",
    "    df['description_full'] = None\n",
    "\n",
    "# Reanudar desde checkpoint si existe\n",
    "start_index = 0\n",
    "if os.path.exists(CHECKPOINT_FILE):\n",
    "    with open(CHECKPOINT_FILE, \"r\") as f:\n",
    "        start_index = int(f.read().strip())\n",
    "    print(f\"‚è∏Ô∏è Reanudando scraping desde √≠ndice {start_index}\")\n",
    "else:\n",
    "    print(f\"üîÅ Iniciando scraping desde el √≠ndice {start_index}\")\n",
    "\n",
    "# Si ya existe el archivo de salida, cargarlo y conservar lo anterior\n",
    "if os.path.exists(OUTPUT_FILE):\n",
    "    df_prev = pd.read_csv(OUTPUT_FILE)\n",
    "    df.loc[:len(df_prev)-1, 'description_full'] = df_prev['description_full']\n",
    "    print(f\"üìÇ Archivo existente {OUTPUT_FILE} cargado con {len(df_prev)} filas ya scrapeadas.\")\n",
    "\n",
    "# Lista de proxies (reemplazar con proxies reales)\n",
    "PROXIES = [\n",
    "    {\"host\": \"194.67.37.90\", \"port\": \"3128\", \"username\": \"\", \"password\": \"\"},\n",
    "    {\"host\": \"residential.zenrows.com\", \"port\": \"8001\", \"username\": \"user1\", \"password\": \"pass1\"},\n",
    "    {\"host\": \"proxy.soax.com\", \"port\": \"10000\", \"username\": \"user2\", \"password\": \"pass2\"},\n",
    "    {\"host\": \"geo.rotating.proxy.shifter.io\", \"port\": \"35000\", \"username\": \"user3\", \"password\": \"pass3\"},\n",
    "    {\"host\": \"pr.oxylabs.io\", \"port\": \"7777\", \"username\": \"user4\", \"password\": \"pass4\"},\n",
    "    {\"host\": \"51.159.115.233\", \"port\": \"3128\", \"username\": \"\", \"password\": \"\"},\n",
    "    {\"host\": \"51.15.147.172\", \"port\": \"3128\", \"username\": \"\", \"password\": \"\"}\n",
    "]\n",
    "\n",
    "# Lista de user-agents\n",
    "USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)...\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)...\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64)...\",\n",
    "]\n",
    "\n",
    "# Inicializador de driver\n",
    "def init_driver(proxy, user_agent):\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--incognito\")\n",
    "    options.add_argument(f\"user-agent={user_agent}\")\n",
    "\n",
    "    prefs = {\n",
    "        \"profile.managed_default_content_settings.images\": 2,\n",
    "        \"profile.managed_default_content_settings.stylesheets\": 2,\n",
    "        \"profile.managed_default_content_settings.fonts\": 2\n",
    "    }\n",
    "    options.add_experimental_option(\"prefs\", prefs)\n",
    "\n",
    "    plugin_path = create_proxy_auth_extension(\n",
    "        proxy_host=proxy[\"host\"],\n",
    "        proxy_port=proxy[\"port\"],\n",
    "        proxy_username=proxy[\"username\"],\n",
    "        proxy_password=proxy[\"password\"]\n",
    "    )\n",
    "    options.add_extension(plugin_path)\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    return driver, plugin_path\n",
    "\n",
    "# Scraping function\n",
    "def scrape_job_description_selenium(driver, url):\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        WebDriverWait(driver, 2.5).until(\n",
    "            EC.presence_of_element_located(\n",
    "                (By.CSS_SELECTOR, 'section.adp-body.mx-4.text-sm.md\\\\:mx-0.md\\\\:text-base')\n",
    "            )\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            show_more_button = driver.find_element(\n",
    "                By.CSS_SELECTOR,\n",
    "                \"a.font-bold.bg-white.inline-block.px-8.pt-4.text-adzuna-green-500.rounded-t-lg.hover\\\\:underline\"\n",
    "            )\n",
    "            show_more_button.click()\n",
    "            print(\"üîò 'Show Full Description' presionado.\")\n",
    "            time.sleep(random.uniform(0.2, 1))\n",
    "        except NoSuchElementException:\n",
    "            print(\"‚ö†Ô∏è Bot√≥n 'Show Full Description' no encontrado.\")\n",
    "\n",
    "        section = driver.find_element(By.CSS_SELECTOR, 'section.adp-body.mx-4.text-sm.md\\\\:mx-0.md\\\\:text-base')\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView();\", section)\n",
    "        time.sleep(random.uniform(0.5, 1.5))\n",
    "\n",
    "        description_html = section.get_attribute('innerHTML')\n",
    "        description_text = description_html.replace('<br>', '\\n')\n",
    "\n",
    "        if not description_text.strip():\n",
    "            elements = section.find_elements(By.CSS_SELECTOR, \"p, li\")\n",
    "            if not elements:\n",
    "                print(f\"‚ö†Ô∏è No se encontraron <p> o <li> en {url}\")\n",
    "                return None\n",
    "            description_text = \"\\n\".join([el.text.strip() for el in elements if el.text.strip()])\n",
    "\n",
    "        if not description_text.strip():\n",
    "            print(f\"‚ö†Ô∏è Descripci√≥n vac√≠a en {url}\")\n",
    "            return None\n",
    "\n",
    "        return description_text\n",
    "\n",
    "    except (TimeoutException, NoSuchElementException, ValueError) as e:\n",
    "        print(f\"‚ö†Ô∏è Error en {url}: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error inesperado: {e}\")\n",
    "        return None\n",
    "\n",
    "# Variables de control\n",
    "driver = None\n",
    "plugin_path = None\n",
    "restart_interval = 50\n",
    "counter = 0\n",
    "\n",
    "# Loop principal de scraping\n",
    "for idx in range(start_index, len(df)):\n",
    "    url = df.at[idx, \"redirect_url\"]\n",
    "    if pd.notnull(url):\n",
    "        if counter % restart_interval == 0:\n",
    "            if driver:\n",
    "                driver.quit()\n",
    "                if plugin_path and os.path.exists(plugin_path):\n",
    "                    os.remove(plugin_path)\n",
    "            proxy = random.choice(PROXIES)\n",
    "            user_agent = random.choice(USER_AGENTS)\n",
    "            driver, plugin_path = init_driver(proxy, user_agent)\n",
    "            print(f\"üîÅ Driver reiniciado con proxy {proxy['host']}\")\n",
    "\n",
    "        description = scrape_job_description_selenium(driver, url)\n",
    "        df.at[idx, 'description_full'] = description\n",
    "        print(f\"[{idx}] URL procesada: {'‚úÖ' if description else '‚ùå'}\")\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "        # Guardado parcial\n",
    "        if counter % 500 == 0:\n",
    "            df.to_csv(OUTPUT_FILE, index=False)\n",
    "            with open(CHECKPOINT_FILE, \"w\") as f:\n",
    "                f.write(str(idx + 1))\n",
    "            print(f\"üíæ Guardado parcial en √≠ndice {idx}\")\n",
    "\n",
    "        time.sleep(random.uniform(0.2, 1))\n",
    "\n",
    "# Limpieza final\n",
    "if driver:\n",
    "    driver.quit()\n",
    "if plugin_path and os.path.exists(plugin_path):\n",
    "    os.remove(plugin_path)\n",
    "\n",
    "df.to_csv(OUTPUT_FILE, index=False)\n",
    "with open(CHECKPOINT_FILE, \"w\") as f:\n",
    "    f.write(str(len(df)))\n",
    "\n",
    "print(\"‚úÖ Scraping finalizado y guardado.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
